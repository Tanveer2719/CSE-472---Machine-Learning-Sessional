{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logisticRegression as lgr\n",
    "import importlib\n",
    "importlib.reload(lgr)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.metrics import confusion_matrix,f1_score, roc_auc_score, average_precision_score, precision_score,recall_score \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the duplicated rows\n",
    "def drop_duplicates(dataframe):\n",
    "    dataframe.drop_duplicates(inplace=True)\n",
    "    return dataframe\n",
    "\n",
    "# separate the data frame into features and target\n",
    "def create_feature_target(dataframe, column=''):\n",
    "    features = dataframe.drop(column, axis=1)\n",
    "    target = dataframe[column]\n",
    "    return features, target\n",
    "\n",
    "# label encode the target\n",
    "def label_encoding_target_helper(target, cols=[]):\n",
    "    le = LabelEncoder()\n",
    "    target = le.fit_transform(target.values.ravel())\n",
    "    target = pd.DataFrame(target, columns=cols)\n",
    "    return target\n",
    "\n",
    "# label encode the feature\n",
    "def label_encoding_features_helper(features):\n",
    "    # Label Encode binary columns\n",
    "    binary_cols = [c for c in features.columns if features[c].dtype not in ['int64', 'float64'] and features[c].nunique() == 2]\n",
    "    binary_cols\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    for c in binary_cols:\n",
    "        features[c] = le.fit_transform(features[c])\n",
    "\n",
    "    return features\n",
    "\n",
    "# one hot encoding\n",
    "def one_hot_helper(features):\n",
    "    # OneHot encoding\n",
    "    object_cols = features.select_dtypes(include=['object']).columns\n",
    "\n",
    "\n",
    "    # chage the datatype to category\n",
    "    for c in object_cols:\n",
    "        features[c] = features[c].astype('category')\n",
    "\n",
    "    features = pd.get_dummies(features)\n",
    "    features = features.astype('int')\n",
    "    return features\n",
    "\n",
    "# scaling helper\n",
    "def scaling_helper(features, scaler='minmax'):\n",
    "    # we separate the binary cols\n",
    "    binary_cols = [c for c in features.columns if features[c].nunique() == 2]\n",
    "\n",
    "    binary_features = features[binary_cols]\n",
    "    continous_features = features.drop(columns=binary_cols)\n",
    "\n",
    "    if scaler == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    else:\n",
    "        scaler = MinMaxScaler()\n",
    "    \n",
    "    scaled_features = scaler.fit_transform(continous_features)\n",
    "    continous_features = pd.DataFrame(scaled_features, columns=continous_features.columns)\n",
    "\n",
    "    # reset the index\n",
    "    binary_features.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    #concat the two matrices\n",
    "    features = pd.concat([binary_features,continous_features], axis=1)\n",
    "    return features\n",
    "\n",
    "# correlation helper\n",
    "def correlation_helper(features, target, target_col_name):\n",
    "    target_series = target[target_col_name]\n",
    "    correlations = features.corrwith(target_series)\n",
    "    return pd.DataFrame({'Feature': correlations.index, 'Correlation': correlations.values})\n",
    "\n",
    "def information_gain_helper(features, target):\n",
    "    mi = mutual_info_classif(features,target)\n",
    "    return pd.DataFrame({'Feature': features.columns, 'Information Gain': mi})\n",
    "\n",
    "def get_top_features(correlations, info_gain, top=40):\n",
    "    # Combine DataFrames\n",
    "    combined_df = info_gain.merge(correlations, on='Feature', how='inner')\n",
    "    \n",
    "    # Calculate a combined score based on absolute values\n",
    "    combined_df['Absolute Info Gain'] = combined_df['Information Gain'].abs()\n",
    "    combined_df['Absolute Correlation'] = combined_df['Correlation'].abs()\n",
    "    combined_df['Combined Score'] = combined_df['Absolute Info Gain'] + combined_df['Absolute Correlation']\n",
    "    \n",
    "    # Sort by combined score\n",
    "    combined_df = combined_df.sort_values(by='Combined Score', ascending=False)\n",
    "    \n",
    "    #return the top features\n",
    "    return combined_df.head(top)['Feature'] \n",
    "\n",
    "\n",
    "# Data splitter\n",
    "def split_data(features, target, test_size=0.2, random_state=42):\n",
    "    \n",
    "    # Check if the target is a pandas DataFrame/Series\n",
    "    if isinstance(target, pd.DataFrame) or isinstance(target, pd.Series):\n",
    "        target = target.values.ravel()  # Convert to 1D numpy array\n",
    "    \n",
    "    # If it's already a numpy array and has more than 1 dimension, flatten it\n",
    "    elif isinstance(target, np.ndarray) and target.ndim > 1:\n",
    "        target = target.ravel()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    y_train = pd.Series(y_train).reset_index(drop=True).to_numpy().ravel()\n",
    "    y_test = pd.Series(y_test).reset_index(drop=True).to_numpy().ravel()\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('../Program_Data/creditcard.csv')\n",
    "\n",
    "#  no null value in the cells\n",
    "\n",
    "\n",
    "# drop the duplicated rows\n",
    "# since no positive values in the duplicated rows, they can also be deleted\n",
    "dataframe = drop_duplicates(dataframe)\n",
    "\n",
    "\n",
    "# take 20,000 randomly from the negative class and take all the positive class data\n",
    "negative_class = dataframe[dataframe['Class']==0]\n",
    "positive_class = dataframe[dataframe['Class']==1]\n",
    "\n",
    "negative_class = negative_class.sample(20000, random_state=42)\n",
    "\n",
    "# concat the two class and reshuffle them to mix the classes\n",
    "dataframe = pd.concat([negative_class, positive_class])\n",
    "dataframe = dataframe.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# separate into features and target\n",
    "features, target = create_feature_target(dataframe, column='Class')\n",
    "\n",
    "\n",
    "# scale the features\n",
    "features = scaling_helper(features, scaler='minmax')\n",
    "\n",
    "# feature selection\n",
    "# since there are only 30 columns, we avoid the feature selection part\n",
    "\n",
    "# split the dataset to train and test\n",
    "X_train, X_test, y_train, y_test = split_data(features, target)\n",
    "\n",
    "# create the validation set\n",
    "X_train, X_validation, y_train, y_validation = split_data(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**********Before fitting\n",
      "X_train.shape = (13102, 30)   y_train.shape = (13102,)\n",
      "X_validation.shape = (3276, 30)     y_validation.shape = (3276,)\n",
      "X_test.shape = (4095, 30)     y_test.shape = (4095,)\n",
      "0s in train:12793  1s in train: 309\n",
      "0s in test:3988  1s in test: 107\n",
      "class_weight = {0: 0.0929, 1: 0.9081}\n",
      "3995\n",
      "100\n",
      "The accuracy of skLearn.LogisticRegression : 0.99\n",
      "The precision of skLearn.LogisticRegression : 0.76\n",
      "The recall of skLearn.LogisticRegression : 0.71\n",
      "The f1 of skLearn.LogisticRegression : 0.73\n",
      "The auroc of skLearn.LogisticRegression : 0.85\n",
      "The aupr of skLearn.LogisticRegression : 0.55\n",
      "The sensitivity of skLearn.LogisticRegression : 0.99\n",
      "The specificity of skLearn.LogisticRegression : 0.76\n"
     ]
    }
   ],
   "source": [
    "\n",
    "weight_0 = 0.0929\n",
    "weight_1 = 0.9081\n",
    "\n",
    "class_weight = {0:weight_0, 1:weight_1}\n",
    "\n",
    "print(\"\\n**********Before fitting\")\n",
    "print(f'X_train.shape = {X_train.shape}   y_train.shape = {y_train.shape}')\n",
    "print(f'X_validation.shape = {X_validation.shape}     y_validation.shape = {y_validation.shape}')\n",
    "print(f'X_test.shape = {X_test.shape}     y_test.shape = {y_test.shape}')\n",
    "print(f'0s in train:{np.sum(y_train == 0)}  1s in train: {np.sum(y_train==1)}' )\n",
    "print(f'0s in test:{np.sum(y_test == 0)}  1s in test: {np.sum(y_test==1)}' )\n",
    "print(f'class_weight = {class_weight}')\n",
    "\n",
    "\n",
    "model = lgr.CustomLogisticRegression(class_weight=class_weight)\n",
    "model.custom_fit(X_train, y_train)\n",
    "y_pred = model.custom_predict(X_test, threshold=0.4095)\n",
    "print(np.sum(y_pred == 0))\n",
    "print(np.sum(y_pred == 1))\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "auroc = roc_auc_score(y_test, y_pred)  # Use predicted probabilities for AUROC\n",
    "aupr = average_precision_score(y_test, y_pred)  # Precision-Recall AUC\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "tp, fp, fn, tn = conf_matrix.ravel()\n",
    "sensitivity = tp/(tp+fn) if (tp+fn)>0 else 0\n",
    "specificity = tn/(tn+fp) if (tn+fp)>0 else 0\n",
    "\n",
    "\n",
    "print(f'The accuracy of skLearn.LogisticRegression : {accuracy:.2f}')\n",
    "print(f'The precision of skLearn.LogisticRegression : {precision:.2f}')\n",
    "print(f'The recall of skLearn.LogisticRegression : {recall:.2f}')\n",
    "print(f'The f1 of skLearn.LogisticRegression : {f1:.2f}')\n",
    "print(f'The auroc of skLearn.LogisticRegression : {auroc:.2f}')\n",
    "print(f'The aupr of skLearn.LogisticRegression : {aupr:.2f}')\n",
    "print(f'The sensitivity of skLearn.LogisticRegression : {sensitivity:.2f}')\n",
    "print(f'The specificity of skLearn.LogisticRegression : {specificity:.2f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Solution",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
