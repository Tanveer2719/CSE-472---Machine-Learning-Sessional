{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orkbObBCghAX"
      },
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "fDt3QY7BghAb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import copy\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# estimated time : 1m 23s\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZadmgyjghAe"
      },
      "source": [
        "### Global Params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "IxRgWvvmghAf"
      },
      "outputs": [],
      "source": [
        "num_iter = 10\n",
        "batch_size = 128\n",
        "learning_rates = [0.005, .001, .0005, .0001]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5fUigYDghAg"
      },
      "source": [
        "### Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FslSa0g8ghAg",
        "outputId": "294c1529-6376-4a35-9c51-ee14f84b803f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x.shape: (48000, 784) y.shape: (48000, 10)\n",
            "x_validation.shape: (12000, 784) y_validation.shape: (12000, 10)\n",
            "x_test.shape: (10000, 784) y_test.shape: (10000, 10)\n"
          ]
        }
      ],
      "source": [
        "def one_hot(y, nclass):\n",
        "    n = y.size\n",
        "    Y = np.zeros((n,nclass))\n",
        "    Y[np.arange(n),y] = 1\n",
        "\n",
        "    return Y\n",
        "\n",
        "def train_test_split(X,y,ratio=0.2,shuffle = True):\n",
        "\n",
        "    m = X.shape[0]\n",
        "    if shuffle==True:\n",
        "        index = np.arange(m)\n",
        "        np.random.shuffle(index)\n",
        "        X = X[index]\n",
        "        y = y[index]\n",
        "\n",
        "    split = np.round((1-ratio)*m).astype(int)\n",
        "\n",
        "    X_train = X[:split]\n",
        "    y_train = y[:split]\n",
        "\n",
        "    X_test = X[split:]\n",
        "    y_test = y[split:]\n",
        "\n",
        "    return X_train, y_train,  X_test, y_test\n",
        "\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "\n",
        "# load the training dataset\n",
        "train_dataset = datasets.FashionMNIST(root='./data', train =True, transform = transform, download=True)\n",
        "\n",
        "#load the test dataset\n",
        "test_dataset = datasets.FashionMNIST(root='./data', train =False, transform = transform, download=True)\n",
        "\n",
        "# Using list comprehension to convert images and labels into tensors\n",
        "x_train = torch.stack([image for image, label in train_dataset])  # Stack all images\n",
        "y_train = torch.tensor([label for _, label in train_dataset])     # Create a tensor for labels\n",
        "\n",
        "x_test = torch.stack([image for image, label in test_dataset])  # Stack all images\n",
        "y_test = torch.tensor([label for _, label in test_dataset])     # Create a tensor for labels\n",
        "\n",
        "# Convert to numpy arrays\n",
        "x_train = x_train.numpy()  # Shape will be (60000, 1, 28, 28)\n",
        "y_train = y_train.numpy()  # Shape will be (60000,)\n",
        "\n",
        "x_test = x_test.numpy() # shape (10000, 1, 28, 28)\n",
        "y_test = y_test.numpy()\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)  # Flatten to shape (60000, 784)\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)  # Flatten to shape (10000, 784)\n",
        "\n",
        "\n",
        "X = x_train\n",
        "Y = y_train\n",
        "\n",
        "Y = one_hot(Y, len(np.unique(Y)))\n",
        "y_test = one_hot(y_test, len(np.unique(y_test)))\n",
        "\n",
        "X, Y, x_validation, y_validation = train_test_split(X, Y)\n",
        "\n",
        "print(f'x.shape: {X.shape} y.shape: {Y.shape}')\n",
        "print(f'x_validation.shape: {x_validation.shape} y_validation.shape: {y_validation.shape}')\n",
        "print(f'x_test.shape: {x_test.shape} y_test.shape: {y_test.shape}')\n",
        "\n",
        "\n",
        "\n",
        "# estimated time : 29s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v28cQNB-ghAi"
      },
      "source": [
        "### Batch Normalization\n",
        "\n",
        "Batch Normalization layer that scales and shifts normalized input.\n",
        "The scaling w and shift b are learned by the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "MhRyj6a0ghAj"
      },
      "outputs": [],
      "source": [
        "class BatchNorm:\n",
        "\n",
        "    def __init__(self, input_shape, momentum=0.9, epsilon=1e-7):\n",
        "        \"\"\"\n",
        "        Initializes the BatchNorm layer.\n",
        "\n",
        "        Args:\n",
        "            input_shape (tuple): Shape of the input vector, excluding batch size.\n",
        "            momentum (float, optional): Momentum for running mean/variance. Defaults to 0.9.\n",
        "            epsilon (float, optional): Small constant for numerical stability. Defaults to 1e-7.\n",
        "        \"\"\"\n",
        "        d = input_shape\n",
        "\n",
        "        # Learnable parameters (scale and shift)\n",
        "        self.w = np.ones((1, d))  # Scale parameter (initialized to 1)\n",
        "        self.b = np.zeros((1, d))  # Shift parameter (initialized to 0)\n",
        "\n",
        "        # Gradients for scale and shift parameters\n",
        "        self.dW = np.zeros((1, d))\n",
        "        self.db = np.zeros((1, d))\n",
        "        self.cache = None\n",
        "\n",
        "        # Store running mean and variance for inference\n",
        "        self.running_mean = np.zeros((1, d))\n",
        "        self.running_var = np.ones((1, d))\n",
        "        self.momentum = momentum\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, x, train=True):\n",
        "\n",
        "\n",
        "        if train:\n",
        "            # Calculate batch mean and variance\n",
        "            mu = np.mean(x, axis=0, keepdims=True)\n",
        "            var = np.mean((x - mu)**2, axis=0, keepdims=True)\n",
        "\n",
        "            # Normalize the batch\n",
        "            # (x - mu) / sqrt(var + epsilon)\n",
        "            xmu = x - mu\n",
        "            sqrtvar = np.sqrt(var + self.epsilon)\n",
        "            ivar = 1.0 / sqrtvar\n",
        "            xhat = xmu * ivar\n",
        "\n",
        "            # Update running mean and variance\n",
        "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mu\n",
        "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * var\n",
        "\n",
        "            # Store intermediate values for backpropagation\n",
        "            self.cache = (xhat, xmu, ivar, sqrtvar, var, mu)\n",
        "        else:\n",
        "            # Use running mean and variance during inference\n",
        "            xhat = (x - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
        "\n",
        "        # Scale and shift\n",
        "        # w * xhat + b\n",
        "        out = self.w * xhat + self.b\n",
        "\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "\n",
        "        # Unfold the variables stored in cache during forward pass\n",
        "        xhat, xmu, ivar, sqrtvar, var, mu = self.cache\n",
        "\n",
        "        N, D = dout.shape\n",
        "\n",
        "        # Gradient of scale and shift parameters\n",
        "\n",
        "        # dL / dBeta = ∑ dL/dy\n",
        "        self.db = np.sum(dout, axis=0, keepdims=True)\n",
        "\n",
        "        # dL / dGamma = ∑ dL/dy * x_hat\n",
        "        self.dW = np.sum(dout * xhat, axis=0, keepdims=True)\n",
        "\n",
        "        # Gradient of normalized input\n",
        "\n",
        "        # dL / dx_hat = dL/dy * gamma\n",
        "        dxhat = dout * self.w\n",
        "\n",
        "        # Gradients for batch normalization layer\n",
        "        # dL / dVar = ∑ dL/dx_hat * (x - mean) * -1/2 * (var + epsilon)^(-3/2)\n",
        "        dvar = np.sum(dxhat * xmu, axis=0) * -0.5 * (var + self.epsilon)**-1.5\n",
        "\n",
        "        # dL / dMean = ∑ dL/dx_hat * -1/sqrt(var + epsilon) + dL/dVar * ∑ -2(x - mean) / N\n",
        "        dmu = np.sum(dxhat * -ivar, axis=0) + dvar * np.sum(-2.0 * xmu, axis=0) / N\n",
        "\n",
        "        dx1 = dxhat * ivar\n",
        "        dx2 = dvar * 2.0 * xmu / N\n",
        "        dZ = dx1 + dx2 + dmu / N\n",
        "\n",
        "        return dZ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXEegRoDghAl"
      },
      "source": [
        "### Adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "iiYUqMiughAm"
      },
      "outputs": [],
      "source": [
        "class Adam:\n",
        "    \"\"\"\n",
        "    Our favourite Adam optimization that combines momentum and rmsprop\n",
        "    \"\"\"\n",
        "    def __init__(self, net, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        # params\n",
        "        params = net.get_weights()\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.beta1=beta1\n",
        "        self.beta2=beta2\n",
        "        self.epsilon=epsilon\n",
        "\n",
        "        # init\n",
        "        self.VW = {}\n",
        "        self.Vb = {}\n",
        "        self.SW = {}\n",
        "        self.Sb = {}\n",
        "        for k in params.keys():\n",
        "            self.VW[k] = 0\n",
        "            self.Vb[k] = 0\n",
        "            self.SW[k] = 0\n",
        "            self.Sb[k] = 0\n",
        "    def update(self, net):\n",
        "\n",
        "        params = net.get_weights()\n",
        "        dparams = net.get_dweights()\n",
        "\n",
        "        beta1 = self.beta1\n",
        "        beta2 = self.beta2\n",
        "\n",
        "        for k,(dW,db) in dparams.items():\n",
        "            W,b = params[k]\n",
        "            # momentum\n",
        "            self.VW[k] = beta1*self.VW[k] + (1.-beta1)*dW #dW\n",
        "            self.Vb[k] = beta1*self.Vb[k] + (1.-beta1)*db #db\n",
        "            # rmsprop\n",
        "            self.SW[k] = beta2*self.SW[k] + (1.-beta2)*(dW**2) #dW**2\n",
        "            self.Sb[k] = beta2*self.Sb[k] + (1.-beta2)*(db**2) #db**2\n",
        "\n",
        "            W -= self.learning_rate * self.VW[k]/(np.sqrt(self.SW[k]) + self.epsilon) # W\n",
        "            b -= self.learning_rate * self.Vb[k]/(np.sqrt(self.Sb[k]) + self.epsilon) # b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpV0i3ZtghAn"
      },
      "source": [
        "### Relu\n",
        "\n",
        "During backpropagation, relU propagates the gradient only where x > 0\n",
        "\n",
        "Because, the gradient of Relu is 1 for positive and 0 for negative inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "O-emS0KKghAn"
      },
      "outputs": [],
      "source": [
        "class RelU:\n",
        "    def __init__(self):\n",
        "        self.input = None\n",
        "    def forward(self, x):\n",
        "        self.input = x\n",
        "        return np.maximum(0,x)\n",
        "\n",
        "    def backward(self, dout):\n",
        "\n",
        "        return dout * (self.input > 0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E5jHJlMghAo"
      },
      "source": [
        "### Dropout\n",
        "\n",
        "Randomly setting some neurons to zero during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "_5VQRvglghAo"
      },
      "outputs": [],
      "source": [
        "class Dropout:\n",
        "    def __init__(self, p=0.5):\n",
        "        self.probability = p\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x, train=True):\n",
        "\n",
        "        if train:\n",
        "            # Create a mask with the same shape as the input,\n",
        "            # and set neurons to zero with probability\n",
        "            self.mask = (np.random.rand(*x.shape) < self.probability) / self.probability\n",
        "            return x * self.mask\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "    def backward(self, dout):\n",
        "\n",
        "        # gradient is zero for the neurons that were turned off during forward pass\n",
        "        return dout * self.mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDzm2tvxghAp"
      },
      "source": [
        "### SoftMax With Cross-Entropy loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "8OJatp3qghAp"
      },
      "outputs": [],
      "source": [
        "class Loss:\n",
        "    def __init__(self):\n",
        "        self.y = None\n",
        "        self.y_pred = None\n",
        "\n",
        "    def softmax(self, x):\n",
        "        '''\n",
        "            x: np.ndarray\n",
        "        '''\n",
        "        exps = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "        return exps / np.sum(exps, axis=-1, keepdims=True)\n",
        "\n",
        "    def cross_entropy(self, y, y_pred):\n",
        "        '''\n",
        "            y: np.ndarray\n",
        "            y_pred: np.ndarray\n",
        "        '''\n",
        "        # Cross entropy loss\n",
        "        # calculate the softmax of the predicted output\n",
        "        y_pred = self.softmax(y_pred)\n",
        "        # calculate the loss\n",
        "        loss = -np.sum(y * np.log(y_pred + 1e-8), axis=-1)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, y, y_pred):\n",
        "        '''\n",
        "            y: np.ndarray , the true labels\n",
        "            y_pred: np.ndarray, the predicted labels\n",
        "        '''\n",
        "        self.y = y\n",
        "        self.y_pred = y_pred\n",
        "        return np.mean(self.cross_entropy(y, y_pred))\n",
        "\n",
        "    def backward(self):\n",
        "        '''\n",
        "            returns the gradient of the loss with respect to the input\n",
        "        '''\n",
        "        return (self.softmax(self.y_pred) - self.y) / self.y.shape[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XU3asCu0ghAp"
      },
      "source": [
        "### Dense Layer\n",
        "A fully connected layer :: y = Wx + b\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "2g_xM5SoghAp"
      },
      "outputs": [],
      "source": [
        "class DenseLayer:\n",
        "    def __init__(self, input_size, output_size):\n",
        "        '''\n",
        "            input_size: int\n",
        "            output_size: int\n",
        "        '''\n",
        "        # he initialization\n",
        "        self.w = np.random.randn(input_size, output_size) * np.sqrt(2/input_size)\n",
        "        self.b = np.zeros((1, output_size))\n",
        "\n",
        "        self.dW = np.zeros_like(self.w)\n",
        "        self.db = np.zeros_like(self.b)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "            x: np.ndarray\n",
        "        '''\n",
        "        self.x = x\n",
        "\n",
        "        # Debugging: Print shapes to verify compatibility\n",
        "        # print(f\"Input shape: {x.shape}, Weights shape: {self.w.shape}, Bias shape: {self.b.shape}\")\n",
        "\n",
        "        return (x @ self.w) + self.b\n",
        "\n",
        "    def backward(self, dout):\n",
        "        '''\n",
        "            dout: np.ndarray\n",
        "        '''\n",
        "        self.dW = self.x.T @ dout\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        return dout @ self.w.T\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFSslAAwghAq"
      },
      "source": [
        "### FNN\n",
        "Fully Connected Feed Forward Neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "5TBQbEYfghAq"
      },
      "outputs": [],
      "source": [
        "class FNN:\n",
        "    def __init__(self):\n",
        "\n",
        "# ############### Model 1 ################\n",
        "#         # input layer\n",
        "#         self.layer1 = DenseLayer(784, 128)\n",
        "#         self.relu = RelU()\n",
        "#         self.bn = BatchNorm(128)\n",
        "#         self.drop = Dropout()\n",
        "\n",
        "#         # Second layer\n",
        "#         self.layer3 = DenseLayer(128, 64)\n",
        "#         self.relu3 = RelU()\n",
        "#         self.bn3 = BatchNorm(64)\n",
        "#         self.drop3 = Dropout()\n",
        "\n",
        "#         # output layer\n",
        "#         self.layer4 = DenseLayer(64, 10)\n",
        "#         self.softmax = Loss()\n",
        "\n",
        "#         self.layers = {\n",
        "#             'l1' : self.layer1,\n",
        "#             'rel1' : self.relu,\n",
        "#             'bn1' : self.bn,\n",
        "#             'drp1' : self.drop,\n",
        "#             'l3' : self.layer3,\n",
        "#             'rel3': self.relu3,\n",
        "#             'bn3': self.bn3,\n",
        "#             'l4' : self.layer4\n",
        "\n",
        "#         }\n",
        "\n",
        "############### Model 2 ################\n",
        "        # input layer\n",
        "        self.layer1 = DenseLayer(784, 512)\n",
        "        self.relu = RelU()\n",
        "        self.bn = BatchNorm(512)\n",
        "        self.drop = Dropout()\n",
        "\n",
        "        # second layer\n",
        "        self.layer2 = DenseLayer(512, 128)\n",
        "        self.relu2 = RelU()\n",
        "        self.bn2 = BatchNorm(128)\n",
        "        self.drop2 = Dropout()\n",
        "\n",
        "        # third layer\n",
        "        self.layer3 = DenseLayer(128, 64)\n",
        "        self.relu3 = RelU()\n",
        "        self.bn3 = BatchNorm(64)\n",
        "        self.drop3 = Dropout()\n",
        "\n",
        "        # output layer\n",
        "        self.layer4 = DenseLayer(64, 10)\n",
        "        self.softmax = Loss()\n",
        "\n",
        "        self.layers = {\n",
        "            'l1' : self.layer1,\n",
        "            'rel1' : self.relu,\n",
        "            'bn1' : self.bn,\n",
        "            'drp1' : self.drop,\n",
        "            'l2': self.layer2,\n",
        "            'rel2': self.relu2,\n",
        "            'bn2': self.bn2,\n",
        "            'drp2': self.drop2,\n",
        "            'l3' : self.layer3,\n",
        "            'rel3': self.relu3,\n",
        "            'bn3': self.bn3,\n",
        "            'l4' : self.layer4\n",
        "\n",
        "        }\n",
        "\n",
        "# ############### Model 3 ################\n",
        "#         # input layer\n",
        "#         self.layer1 = DenseLayer(784, 512)\n",
        "#         self.relu = RelU()\n",
        "#         self.bn = BatchNorm(512)\n",
        "#         self.drop = Dropout()\n",
        "\n",
        "#         # second layer\n",
        "#         self.layer2 = DenseLayer(512, 128)\n",
        "#         self.relu2 = RelU()\n",
        "#         self.bn2 = BatchNorm(128)\n",
        "#         self.drop2 = Dropout()\n",
        "\n",
        "#         # third layer\n",
        "#         self.layer3 = DenseLayer(128, 64)\n",
        "#         self.relu3 = RelU()\n",
        "#         self.bn3 = BatchNorm(64)\n",
        "#         self.drop3 = Dropout()\n",
        "\n",
        "#         # fourth layer\n",
        "#         self.layer4 = DenseLayer(64, 32)\n",
        "#         self.relu4 = RelU()\n",
        "#         self.bn4 = BatchNorm(32)\n",
        "#         self.drop4 = Dropout()\n",
        "\n",
        "#         # output layer\n",
        "#         self.layer5 = DenseLayer(32, 10)\n",
        "#         self.softmax = Loss()\n",
        "\n",
        "#         self.layers = {\n",
        "#             'l1' : self.layer1,\n",
        "#             'rel1' : self.relu,\n",
        "#             'bn1' : self.bn,\n",
        "#             'drp1' : self.drop,\n",
        "#             'l2': self.layer2,\n",
        "#             'rel2': self.relu2,\n",
        "#             'bn2': self.bn2,\n",
        "#             'drp2': self.drop2,\n",
        "#             'l3' : self.layer3,\n",
        "#             'rel3': self.relu3,\n",
        "#             'bn3': self.bn3,\n",
        "#             'drp3': self.drop3,\n",
        "#             'l4' : self.layer4,\n",
        "#             'rel4': self.relu4,\n",
        "#             'bn4': self.bn4,\n",
        "#             'drp4': self.drop4,\n",
        "#             'l5' : self.layer5\n",
        "\n",
        "#         }\n",
        "\n",
        "    def forward(self, X):\n",
        "\n",
        "        # loop through the layers\n",
        "        for layer in self.layers.values():\n",
        "            X = layer.forward(X)\n",
        "\n",
        "        return X\n",
        "\n",
        "    def calculate_loss(self, y, y_pred):\n",
        "        '''\n",
        "            y: np.ndarray -- real values of the target\n",
        "\n",
        "            y_pred: np.ndarray -- predicted values of the target received from the forward pass\n",
        "\n",
        "            Returns:\n",
        "                - float -- the loss\n",
        "                - np.ndarray -- the gradient of the loss with respect to the input\n",
        "\n",
        "        '''\n",
        "\n",
        "        loss =  self.softmax.forward(y, y_pred)\n",
        "        dz = self.softmax.backward()\n",
        "\n",
        "        return loss, dz\n",
        "\n",
        "    def backward(self, dz):\n",
        "        '''\n",
        "            dz: np.ndarray --- The gradient of the loss got from the calculate_loss function\n",
        "        '''\n",
        "\n",
        "        for layer in reversed(self.layers.values()):\n",
        "            dz = layer.backward(dz)\n",
        "\n",
        "        return dz\n",
        "\n",
        "    def get_weights(self):\n",
        "        '''\n",
        "            returns weights to Adam optimizer for updates\n",
        "        '''\n",
        "        weights = {}\n",
        "        for k,layer in self.layers.items():\n",
        "            if hasattr(layer, 'w') and hasattr(layer, 'b'):\n",
        "                weights[k] = (layer.w, layer.b)\n",
        "        return weights\n",
        "\n",
        "    def get_dweights(self):\n",
        "        '''\n",
        "            returns the gradients of the weights to Adam optimizer for updates\n",
        "        '''\n",
        "        dweights = {}\n",
        "        for k,layer in self.layers.items():\n",
        "            if hasattr(layer, 'dW') and hasattr(layer, 'db'):\n",
        "                dweights[k] = (layer.dW, layer.db)\n",
        "        return dweights\n",
        "\n",
        "\n",
        "    def save_model(self, filename):\n",
        "        '''\n",
        "            filename: str -- the name of the file to save the model\n",
        "        '''\n",
        "\n",
        "        weights = self.get_weights()  # Retrieve only weights and biases\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump(weights, f)\n",
        "\n",
        "\n",
        "\n",
        "    def load_model(self, filename):\n",
        "        '''\n",
        "            filename: str -- the name of the file to load the model\n",
        "        '''\n",
        "        with open(filename, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "\n",
        "\n",
        "        for k,layer in self.layers.items():\n",
        "            if k in data:  # Ensure the layer exists in the weights dictionary\n",
        "                    layer.w, layer.b = data[k]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9W6nSOQghAr"
      },
      "source": [
        "### Load From pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "HSblqh3vghAr"
      },
      "outputs": [],
      "source": [
        "# # Load the model\n",
        "# net = FNN(X.shape[1:], out_size=Y.shape[1])\n",
        "# net.load_model(f'model_1905025.pkl')\n",
        "\n",
        "# optimizer = Adam(net, learning_rate=learning_rate)\n",
        "# loss_calculator = Loss()\n",
        "\n",
        "# print('Load completed')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxV1O9SZghAr"
      },
      "source": [
        "### Evaluation of Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "mqT0G-n9ghAr"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(net, X, Y, loss_calculator: Loss):\n",
        "  \"\"\"\n",
        "    Parameters:\n",
        "    - net: The trained neural network model.\n",
        "    - X: Validation feature set.\n",
        "    - Y: Validation target set.\n",
        "    - loss_calculator: Loss function to be used for calculating loss.\n",
        "\n",
        "    Returns:\n",
        "    - validation_loss: The average loss on the validation set.\n",
        "    - validation_accuracy: Accuracy on the validation set.\n",
        "    - macro_f1_score: Macro-F1 score on the validation set.\n",
        "  \"\"\"\n",
        "\n",
        "  A = net.forward(X)\n",
        "\n",
        "  # Calculate loss  (true_labels, predicted_labels)\n",
        "  loss = loss_calculator.forward(Y, A)\n",
        "  y_true = np.argmax(Y, axis=1)\n",
        "  y_pred = np.argmax(A, axis=1)\n",
        "\n",
        "  # Calculate validation accuracy\n",
        "  validation_accuracy = np.sum(y_pred == y_true) / len(y_true)\n",
        "  macro_f1_score = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "  return loss, validation_accuracy, macro_f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot the graphs"
      ],
      "metadata": {
        "id": "VrVSb8OT_2Al"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graph(data_list, lr, param):\n",
        "    \"\"\"\n",
        "    Plots a graph for a given metric.\n",
        "\n",
        "    Args:\n",
        "        data_list (list): List of dictionaries with 'iteration' and 'value' for a specific parameter.\n",
        "        lr (float): Learning rate used during training.\n",
        "        param (str): Parameter name to display on the plot.\n",
        "    \"\"\"\n",
        "    # Extract iterations and values from data_list\n",
        "    iterations = [item['iteration'] for item in data_list]\n",
        "    values = [item['value'] for item in data_list]\n",
        "\n",
        "    # Plotting the data\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(iterations, values, label=f'{param} (lr={lr})')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel(param)\n",
        "    plt.title(f'{param.capitalize()} vs Iteration (Learning Rate: {lr})')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_combined_graphs(data_dict, param):\n",
        "    \"\"\"\n",
        "    Plots combined graphs for a given parameter across different learning rates.\n",
        "\n",
        "    Args:\n",
        "        data_dict (dict): Dictionary where each key is a learning rate and each value\n",
        "                          is a list of dictionaries with 'iteration' and 'value'.\n",
        "        param (str): Parameter name to display on the plot.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    for lr, data_list in data_dict.items():\n",
        "        # Extract iterations and values for each learning rate\n",
        "        iterations = [item['iteration'] for item in data_list]\n",
        "        values = [item['value'] for item in data_list]\n",
        "\n",
        "        # Plot each learning rate series with a label\n",
        "        plt.plot(iterations, values, label=f'lr={lr}')\n",
        "\n",
        "    # Add labels, title, and legend\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel(param)\n",
        "    plt.title(f'{param.capitalize()} vs Iteration for Different Learning Rates')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "GILxuUgw_r9K"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA12kKSrghAs"
      },
      "source": [
        "### Train FNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y84IJwFjghAs",
        "outputId": "c4899565-5eac-40ef-d124-4e83da896689"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning_rate:0.005 iteration:1 train_loss: 0.6292 train_accuracy: 0.7683 val_loss: 0.5206 val_accuracy: 0.8115 val_f1: 0.8063\n",
            "Learning_rate:0.005 iteration:2 train_loss: 0.4754 train_accuracy: 0.8288 val_loss: 0.4649 val_accuracy: 0.8343 val_f1: 0.8337\n",
            "Learning_rate:0.005 iteration:3 train_loss: 0.4448 train_accuracy: 0.8400 val_loss: 0.4432 val_accuracy: 0.8406 val_f1: 0.8389\n",
            "Learning_rate:0.005 iteration:4 train_loss: 0.4208 train_accuracy: 0.8470 val_loss: 0.4385 val_accuracy: 0.8433 val_f1: 0.8439\n",
            "Learning_rate:0.005 iteration:5 train_loss: 0.4126 train_accuracy: 0.8514 val_loss: 0.4246 val_accuracy: 0.8514 val_f1: 0.8507\n",
            "Learning_rate:0.005 iteration:6 train_loss: 0.3975 train_accuracy: 0.8565 val_loss: 0.4256 val_accuracy: 0.8492 val_f1: 0.8490\n",
            "Learning_rate:0.005 iteration:7 train_loss: 0.3857 train_accuracy: 0.8612 val_loss: 0.4152 val_accuracy: 0.8509 val_f1: 0.8509\n",
            "Learning_rate:0.005 iteration:8 train_loss: 0.3807 train_accuracy: 0.8625 val_loss: 0.4100 val_accuracy: 0.8562 val_f1: 0.8562\n",
            "Learning_rate:0.005 iteration:9 train_loss: 0.3745 train_accuracy: 0.8639 val_loss: 0.4119 val_accuracy: 0.8542 val_f1: 0.8551\n",
            "Learning_rate:0.005 iteration:10 train_loss: 0.3671 train_accuracy: 0.8665 val_loss: 0.4045 val_accuracy: 0.8548 val_f1: 0.8561\n",
            "Learning_rate:0.001 iteration:1 train_loss: 0.5987 train_accuracy: 0.7843 val_loss: 0.4850 val_accuracy: 0.8277 val_f1: 0.8264\n",
            "Learning_rate:0.001 iteration:2 train_loss: 0.4552 train_accuracy: 0.8363 val_loss: 0.4578 val_accuracy: 0.8391 val_f1: 0.8373\n",
            "Learning_rate:0.001 iteration:3 train_loss: 0.4277 train_accuracy: 0.8475 val_loss: 0.4391 val_accuracy: 0.8416 val_f1: 0.8423\n",
            "Learning_rate:0.001 iteration:4 train_loss: 0.4091 train_accuracy: 0.8512 val_loss: 0.4174 val_accuracy: 0.8523 val_f1: 0.8530\n",
            "Learning_rate:0.001 iteration:5 train_loss: 0.3889 train_accuracy: 0.8583 val_loss: 0.4060 val_accuracy: 0.8560 val_f1: 0.8561\n",
            "Learning_rate:0.001 iteration:6 train_loss: 0.3809 train_accuracy: 0.8619 val_loss: 0.3988 val_accuracy: 0.8578 val_f1: 0.8579\n",
            "Learning_rate:0.001 iteration:7 train_loss: 0.3742 train_accuracy: 0.8651 val_loss: 0.4087 val_accuracy: 0.8532 val_f1: 0.8545\n",
            "Learning_rate:0.001 iteration:8 train_loss: 0.3658 train_accuracy: 0.8687 val_loss: 0.4044 val_accuracy: 0.8576 val_f1: 0.8571\n",
            "Learning_rate:0.001 iteration:9 train_loss: 0.3620 train_accuracy: 0.8668 val_loss: 0.4038 val_accuracy: 0.8583 val_f1: 0.8595\n",
            "Learning_rate:0.001 iteration:10 train_loss: 0.3567 train_accuracy: 0.8700 val_loss: 0.4048 val_accuracy: 0.8622 val_f1: 0.8626\n",
            "Learning_rate:0.0005 iteration:1 train_loss: 0.6232 train_accuracy: 0.7762 val_loss: 0.4993 val_accuracy: 0.8225 val_f1: 0.8224\n",
            "Learning_rate:0.0005 iteration:2 train_loss: 0.4690 train_accuracy: 0.8305 val_loss: 0.4605 val_accuracy: 0.8343 val_f1: 0.8358\n",
            "Learning_rate:0.0005 iteration:3 train_loss: 0.4323 train_accuracy: 0.8437 val_loss: 0.4322 val_accuracy: 0.8472 val_f1: 0.8470\n",
            "Learning_rate:0.0005 iteration:4 train_loss: 0.4113 train_accuracy: 0.8521 val_loss: 0.4278 val_accuracy: 0.8478 val_f1: 0.8482\n",
            "Learning_rate:0.0005 iteration:5 train_loss: 0.3993 train_accuracy: 0.8551 val_loss: 0.4219 val_accuracy: 0.8514 val_f1: 0.8529\n",
            "Learning_rate:0.0005 iteration:6 train_loss: 0.3833 train_accuracy: 0.8612 val_loss: 0.4151 val_accuracy: 0.8546 val_f1: 0.8551\n",
            "Learning_rate:0.0005 iteration:7 train_loss: 0.3742 train_accuracy: 0.8626 val_loss: 0.4001 val_accuracy: 0.8568 val_f1: 0.8577\n",
            "Learning_rate:0.0005 iteration:8 train_loss: 0.3643 train_accuracy: 0.8671 val_loss: 0.4013 val_accuracy: 0.8584 val_f1: 0.8595\n",
            "Learning_rate:0.0005 iteration:9 train_loss: 0.3643 train_accuracy: 0.8673 val_loss: 0.3927 val_accuracy: 0.8592 val_f1: 0.8591\n",
            "Learning_rate:0.0005 iteration:10 train_loss: 0.3542 train_accuracy: 0.8695 val_loss: 0.3889 val_accuracy: 0.8587 val_f1: 0.8601\n",
            "Learning_rate:0.0001 iteration:1 train_loss: 0.8202 train_accuracy: 0.7229 val_loss: 0.6101 val_accuracy: 0.7885 val_f1: 0.7883\n",
            "Learning_rate:0.0001 iteration:2 train_loss: 0.5767 train_accuracy: 0.8007 val_loss: 0.5425 val_accuracy: 0.8142 val_f1: 0.8144\n",
            "Learning_rate:0.0001 iteration:3 train_loss: 0.5200 train_accuracy: 0.8189 val_loss: 0.5117 val_accuracy: 0.8197 val_f1: 0.8198\n",
            "Learning_rate:0.0001 iteration:4 train_loss: 0.4830 train_accuracy: 0.8310 val_loss: 0.4843 val_accuracy: 0.8331 val_f1: 0.8333\n",
            "Learning_rate:0.0001 iteration:5 train_loss: 0.4606 train_accuracy: 0.8379 val_loss: 0.4678 val_accuracy: 0.8340 val_f1: 0.8340\n",
            "Learning_rate:0.0001 iteration:6 train_loss: 0.4442 train_accuracy: 0.8432 val_loss: 0.4515 val_accuracy: 0.8379 val_f1: 0.8383\n",
            "Learning_rate:0.0001 iteration:7 train_loss: 0.4303 train_accuracy: 0.8469 val_loss: 0.4427 val_accuracy: 0.8411 val_f1: 0.8417\n",
            "Learning_rate:0.0001 iteration:8 train_loss: 0.4197 train_accuracy: 0.8505 val_loss: 0.4228 val_accuracy: 0.8492 val_f1: 0.8497\n",
            "Learning_rate:0.0001 iteration:9 train_loss: 0.4056 train_accuracy: 0.8571 val_loss: 0.4229 val_accuracy: 0.8526 val_f1: 0.8530\n",
            "Learning_rate:0.0001 iteration:10 train_loss: 0.3979 train_accuracy: 0.8591 val_loss: 0.4175 val_accuracy: 0.8524 val_f1: 0.8529\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(12)\n",
        "\n",
        "num_batch = len(X)//batch_size\n",
        "m = num_batch*batch_size   # apply cutoff\n",
        "\n",
        "\n",
        "combined_train_loss = {}\n",
        "combined_train_accuracy = {}\n",
        "combined_val_loss = {}\n",
        "combined_val_accuracy = {}\n",
        "combined_val_f1 = {}\n",
        "\n",
        "best_model = None\n",
        "best_f1 = 0\n",
        "\n",
        "\n",
        "for learning_rate in learning_rates:\n",
        "  # Train mini batch\n",
        "  net = FNN()\n",
        "\n",
        "  optimizer = Adam(net,learning_rate=learning_rate)\n",
        "\n",
        "  loss_calculator = Loss()\n",
        "\n",
        "\n",
        "  # create a list of pairs\n",
        "  train_loss_list = []\n",
        "  train_accuracy_list = []\n",
        "  val_loss_list = []\n",
        "  val_accuracy_list = []\n",
        "  val_f1_list = []\n",
        "\n",
        "\n",
        "  for i in range(num_iter):\n",
        "\n",
        "      permutation = np.random.permutation(m)\n",
        "      train_accuracy = 0\n",
        "      train_loss = 0\n",
        "\n",
        "      for j in range(0,m,batch_size):\n",
        "\n",
        "          indices = permutation[j:j+batch_size]\n",
        "          X_batch, Y_batch = X[indices], Y[indices]\n",
        "\n",
        "          A = net.forward(X_batch)\n",
        "\n",
        "          loss, dZ  = net.calculate_loss(Y_batch, A)\n",
        "\n",
        "          dZ = net.backward(dZ)\n",
        "\n",
        "          optimizer.update(net)\n",
        "\n",
        "          train_loss += loss\n",
        "          train_accuracy += np.sum(np.argmax(A,axis=1)==np.argmax(Y_batch,axis=1))/len(X_batch)\n",
        "\n",
        "      train_loss /= num_batch\n",
        "      train_accuracy /= num_batch\n",
        "\n",
        "      val_loss, val_accu, val_f1 = evaluate_model(net, x_validation, y_validation, loss_calculator)\n",
        "\n",
        "      # update the best model and f1 score\n",
        "      if val_f1 > best_f1:\n",
        "          best_f1 = val_f1\n",
        "          best_model = copy.deepcopy(net)\n",
        "\n",
        "      print(f'Learning_rate:{learning_rate} iteration:{i+1} train_loss: {train_loss:.4f} train_accuracy: {train_accuracy:.4f} val_loss: {val_loss:.4f} val_accuracy: {val_accu:.4f} val_f1: {val_f1:.4f}')\n",
        "\n",
        "      # Store metrics as {iteration, value} pairs\n",
        "      train_loss_list.append({'iteration': i+1, 'value': train_loss})\n",
        "      train_accuracy_list.append({'iteration': i+1, 'value': train_accuracy})\n",
        "      val_loss_list.append({'iteration': i+1, 'value': val_loss})\n",
        "      val_accuracy_list.append({'iteration': i+1, 'value': val_accu})\n",
        "      val_f1_list.append({'iteration': i+1, 'value': val_f1})\n",
        "\n",
        "  # Save lists in combined dictionaries with learning rate as the key\n",
        "  combined_train_loss[learning_rate] = train_loss_list\n",
        "  combined_train_accuracy[learning_rate] = train_accuracy_list\n",
        "  combined_val_loss[learning_rate] = val_loss_list\n",
        "  combined_val_accuracy[learning_rate] = val_accuracy_list\n",
        "  combined_val_f1[learning_rate] = val_f1_list\n",
        "  # plot_graph(train_loss_list, learning_rate, 'train_loss')\n",
        "  # plot_graph(train_accuracy_list, learning_rate, 'train_accuracy')\n",
        "  # plot_graph(val_loss_list, learning_rate, 'val_loss')\n",
        "  # plot_graph(val_accuracy_list, learning_rate, 'val_accuracy')\n",
        "  # plot_graph(val_f1_list, learning_rate, 'val_f1')\n",
        "\n",
        "\n",
        "# # Plot combined graphs for each metric\n",
        "# plot_combined_graphs(combined_train_loss, 'train_loss')\n",
        "# plot_combined_graphs(combined_train_accuracy, 'train_accuracy')\n",
        "# plot_combined_graphs(combined_val_loss, 'val_loss')\n",
        "# plot_combined_graphs(combined_val_accuracy, 'val_accuracy')\n",
        "# plot_combined_graphs(combined_val_f1, 'val_f1')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtUvPAaqghAs"
      },
      "source": [
        "### Test FNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABnJQWOKghAs",
        "outputId": "a3a74bd0-8439-46a8-d5c2-b869a91c10d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_loss: 0.4180 test_accuracy: 0.8531 test_f1: 0.8523\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_accu, test_f1 = evaluate_model(best_model, x_test, y_test, loss_calculator)\n",
        "print(f'test_loss: {test_loss:.4f} test_accuracy: {test_accu:.4f} test_f1: {test_f1:.4f}')\n",
        "\n",
        "# estimated time : 7.6s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhiY48o_ghAs"
      },
      "source": [
        "### Get the confusion matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6z4QHrIghAt",
        "outputId": "41382202-c0c9-4d51-da5d-740aac7ac63d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix for this model: \n",
            "[[ 953    4   27   76    4    2  141    0   11    0]\n",
            " [   4 1150    3   20    2    0    0    0    1    0]\n",
            " [  10    7  953   13  177    0   86    0    6    0]\n",
            " [  15   14   16 1064   48    0   21    0    1    0]\n",
            " [   2    6   85   44 1011    0   58    0    7    0]\n",
            " [   0    0    0    1    0 1097    1   60    3   19]\n",
            " [ 149    3  136   59  114    0  741    0   15    0]\n",
            " [   0    0    0    0    0   26    0 1104    0   29]\n",
            " [   3    3   10    8    9    5   13    5 1125    2]\n",
            " [   0    1    0    0    0   16    0   89    0 1112]]\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "confusion matrix: significance\n",
        "    - The diagonal elements represent the number of points for which the predicted label\n",
        "        is equal to the true label\n",
        "    - The off-diagonal elements are those that are mislabeled by the classifier\n",
        "'''\n",
        "\n",
        "y_pred = best_model.forward(x_validation)\n",
        "confusion_mat = confusion_matrix(np.argmax(y_validation, axis=1), np.argmax(y_pred, axis=1))\n",
        "print(f'Confusion matrix for this model: \\n{confusion_mat}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBieXavSghAt"
      },
      "source": [
        "### Store in pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "zmzzSilighAt"
      },
      "outputs": [],
      "source": [
        "weights = best_model.get_weights()\n",
        "# save weights in a pickle file\n",
        "with open('model_1905025.pkl', 'wb') as f:\n",
        "    pickle.dump(weights, f)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}